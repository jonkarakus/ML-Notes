{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fedbe9",
   "metadata": {},
   "source": [
    "# Dynamic Quantization Notes\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "FP32 → INT8 conversion where:\n",
    "- **Weights**: quantized offline (pre-computed)\n",
    "- **Activations**: quantized at runtime (computed during inference)\n",
    "\n",
    "No calibration data needed - quick to deploy but has runtime overhead.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "**Offline (model load time):**\n",
    "1. Quantize weights FP32 → INT8\n",
    "2. Compute scale & zero-point per layer\n",
    "3. Store quantized weights in model\n",
    "\n",
    "**Runtime (inference):**\n",
    "1. Compute activation scale/zero-point dynamically per batch\n",
    "2. Quantize activations FP32 → INT8\n",
    "3. Run INT8 ops\n",
    "4. Slight overhead from dynamic computation\n",
    "\n",
    "## Math\n",
    "\n",
    "```\n",
    "val_fp32 = scale × (val_int8 - zero_point)\n",
    "\n",
    "scale = max(|range_max|, |range_min|) × 2 / (quant_range_max - quant_range_min)\n",
    "```\n",
    "\n",
    "Zero-point must exactly represent FP32 zero (critical for zero-padding in CNNs).\n",
    "\n",
    "## Calibration Data\n",
    "\n",
    "**Don't need it.** This is the key advantage of dynamic quantization.\n",
    "\n",
    "With static quantization, you need to run the model on representative data to figure out the activation ranges. Dynamic skips this - it computes activation ranges on-the-fly during inference instead.\n",
    "\n",
    "Trade-off: no calibration = faster setup, but runtime overhead from computing ranges dynamically.\n",
    "\n",
    "## When to Use\n",
    "\n",
    "✅ Good for:\n",
    "- CPU inference\n",
    "- No calibration data available\n",
    "- Quick deployment/prototyping\n",
    "- Activations with varying distributions\n",
    "\n",
    "❌ Not ideal for:\n",
    "- GPU/TPU (use static instead)\n",
    "- Production where max performance needed\n",
    "- When there is good calibration data\n",
    "\n",
    "## Example Implementation\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=\"model.onnx\",\n",
    "    model_output=\"model_quant_dynamic.onnx\",\n",
    "    weight_type=QuantType.QInt8  # or QUInt8\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Model must be **opset 10+** (recommend opset 13+)\n",
    "- Two quantized formats exist:\n",
    "  - **QOperator**: QLinearConv, MatMulInteger, etc.\n",
    "  - **QDQ**: Quantize/DeQuantize ops\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "**Data formats (CPU only supports U8X8):**\n",
    "- U8U8 (activation:uint8, weight:uint8) - processes 6 rows/time\n",
    "- U8S8 (activation:uint8, weight:int8) - uses VPMADDUBSW, processes 4 rows\n",
    "\n",
    "Try U8U8 first, then U8S8.\n",
    "\n",
    "**Hardware:**\n",
    "- Best: AVX2, AVX-512, ARM64 CPUs\n",
    "- GPU: not ideal, use static quantization instead\n",
    "\n",
    "## Benchmarking\n",
    "\n",
    "```python\n",
    "import time, numpy as np, onnxruntime as ort\n",
    "\n",
    "def benchmark(model_path, input_shape, num_runs=100):\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    x = np.random.rand(*input_shape).astype(np.float32)\n",
    "    \n",
    "    session.run(None, {input_name: x})  # warm-up\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        session.run(None, {input_name: x})\n",
    "    \n",
    "    return (time.time() - start) / num_runs\n",
    "\n",
    "# Compare\n",
    "print(f\"Original: {benchmark('model.onnx', (1,3,224,224)):.4f}s\")\n",
    "print(f\"Quantized: {benchmark('model_quant.onnx', (1,3,224,224)):.4f}s\")\n",
    "```\n",
    "\n",
    "## vs Static Quantization\n",
    "\n",
    "| | Dynamic | Static |\n",
    "|---|---|---|\n",
    "| Calibration | No | Yes |\n",
    "| Activation quant | Runtime | Offline |\n",
    "| Speed | Good | Better |\n",
    "| Accuracy | Good | Better |\n",
    "| Setup | Easy | Complex |\n",
    "| Best for | CPU, prototyping | GPU/TPU, production |\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- CPU-optimized, don't expect GPU gains\n",
    "- Always validate accuracy on test set\n",
    "- Check model file size reduced (~4x)\n",
    "- Opset must be 10+\n",
    "- If need max performance → use static instead\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```bash\n",
    "pip install onnxruntime\n",
    "```\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\"model.onnx\", \"model_quant.onnx\", weight_type=QuantType.QInt8)\n",
    "```\n",
    "\n",
    "Done. Good for prototyping, move to static for production."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
