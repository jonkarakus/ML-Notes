{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126242e1",
   "metadata": {},
   "source": [
    "# Static Quantization Notes\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "Both weights AND activations quantized offline using calibration data. Better performance than dynamic, but needs more setup.\n",
    "\n",
    "- Pre-compute quantization params for everything\n",
    "- Integer-only inference (no FP ops at runtime)\n",
    "- Ideal for CNNs in production\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "```\n",
    "FP32 model → Prune → Optimize → Calibrate → Quantize → INT8 model\n",
    "```\n",
    "\n",
    "## Step 1: Pruning (Optional but Recommended)\n",
    "\n",
    "**What is pruning?**\n",
    "Remove unimportant weights by setting them to zero. Common approach: magnitude-based pruning - if `|weight| < threshold`, set it to 0.\n",
    "\n",
    "**Why prune before quantizing?**\n",
    "- Fewer non-zero params → smaller quantization range\n",
    "- Better activation distributions (less noise from tiny weights)\n",
    "- Combined with quantization = massive compression (sparse + low precision)\n",
    "- Model learns to compensate for pruned weights during training (if doing iterative pruning)\n",
    "\n",
    "**Typical workflow:**\n",
    "1. Train model normally\n",
    "2. Prune small weights (e.g., threshold=0.01 means drop weights < 1% of max)\n",
    "3. Fine-tune briefly (optional, helps accuracy)\n",
    "4. Quantize\n",
    "\n",
    "```python\n",
    "import onnx, numpy as np\n",
    "\n",
    "def prune_model(model, threshold=0.01):\n",
    "    for layer in model.graph.node:\n",
    "        for attr in layer.attribute:\n",
    "            if attr.name == 'weights':\n",
    "                w = np.frombuffer(attr.tensor.raw_data, dtype=np.float32)\n",
    "                w[abs(w) < threshold] = 0  # Zero out small weights\n",
    "                attr.tensor.raw_data = w.tobytes()\n",
    "    return model\n",
    "\n",
    "model = onnx.load('model.onnx')\n",
    "pruned = prune_model(model, threshold=0.01)\n",
    "onnx.save(pruned, 'model_pruned.onnx')\n",
    "```\n",
    "\n",
    "**Note:** This is basic magnitude pruning. More advanced: structured pruning (remove entire channels/filters), iterative pruning, lottery ticket hypothesis.\n",
    "\n",
    "## Step 2: Optimization\n",
    "\n",
    "Fuse ops, improve graph. Skip for models > 2GB (Protobuf limit).\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "import onnxoptimizer\n",
    "\n",
    "# Pre-process (shape inference + optimization)\n",
    "quant_pre_process(\n",
    "    'model_pruned.onnx',\n",
    "    'model_opt.onnx',\n",
    "    skip_optimization=False,\n",
    "    skip_symbolic_shape=False  # Important for transformers\n",
    ")\n",
    "\n",
    "# Or use onnxoptimizer directly\n",
    "passes = onnxoptimizer.get_fuse_and_elimination_passes()\n",
    "opt_model = onnxoptimizer.optimize(model, passes)\n",
    "```\n",
    "\n",
    "## Calibration Data\n",
    "\n",
    "**What is it?**\n",
    "Representative subset of real data that the model will see in production. Typically 100-1000 samples.\n",
    "\n",
    "**Why needed?**\n",
    "To figure out the range of activations at each layer. Without knowing activation ranges, can't compute proper scale/zero-point values for quantization.\n",
    "\n",
    "**How to choose:**\n",
    "- Must match production data distribution\n",
    "- Include edge cases (dark images, bright images, different object sizes, etc.)\n",
    "- Can use subset of validation set\n",
    "- DON'T just use random noise - will give terrible quantization params\n",
    "\n",
    "**Bad calibration = broken model.**\n",
    "If calibration data doesn't represent real inputs, the computed ranges will be wrong and accuracy tanks.\n",
    "\n",
    "## Step 3: Calibration (Critical!)\n",
    "\n",
    "Run forward passes on calibration data to collect min/max activation values at each layer. These determine scale/zero-point.\n",
    "\n",
    "### Custom CalibrationDataReader\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "import cv2, numpy as np, os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class MyCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_folder, batch_size=1, input_size=(416, 416)):\n",
    "        self.image_folder = image_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.image_files = os.listdir(image_folder)\n",
    "        self.index = 0\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Add your preprocessing here (resize, normalize, etc.)\n",
    "        img = cv2.resize(img, self.input_size)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return img\n",
    "    \n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.image_files):\n",
    "            return None\n",
    "        \n",
    "        batch_paths = [\n",
    "            os.path.join(self.image_folder, self.image_files[i])\n",
    "            for i in range(self.index, min(self.index + self.batch_size, len(self.image_files)))\n",
    "        ]\n",
    "        \n",
    "        # Parallel processing for speed\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            batch = list(executor.map(self.preprocess_image, batch_paths))\n",
    "        \n",
    "        # Shape: (batch, channels, height, width)\n",
    "        batch = np.stack(batch, axis=0)\n",
    "        batch = np.transpose(batch, (0, 3, 1, 2))\n",
    "        \n",
    "        self.index += self.batch_size\n",
    "        return {\"input\": batch}\n",
    "```\n",
    "\n",
    "## Step 4: Quantize\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import quantize_static, QuantType\n",
    "\n",
    "calibration_reader = MyCalibrationDataReader(\n",
    "    'path/to/calibration/images',\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "quantize_static(\n",
    "    model_input='model_opt.onnx',\n",
    "    model_output='model_quant.onnx',\n",
    "    calibration_data_reader=calibration_reader,\n",
    "    activation_type=QuantType.QUInt8,  # uint8 for CPU\n",
    "    weight_type=QuantType.QUInt8,\n",
    "    per_channel=False,                  # True for better accuracy\n",
    "    optimize_model=True,\n",
    "    use_external_data_format=False      # True if > 2GB\n",
    ")\n",
    "```\n",
    "\n",
    "## Math\n",
    "\n",
    "```\n",
    "val_fp32 = scale × (val_int8 - zero_point)\n",
    "\n",
    "scale = max(|range_max|, |range_min|) × 2 / (quant_range_max - quant_range_min)\n",
    "```\n",
    "\n",
    "Calibration determines scale/zero-point by collecting min/max activation values on representative data.\n",
    "\n",
    "## Quantization Types\n",
    "\n",
    "```python\n",
    "QuantType.QInt8       # signed int8\n",
    "QuantType.QUInt8      # unsigned int8 (best for CPU)\n",
    "QuantType.QINT4       # int4 (requires opset 21+)\n",
    "QuantType.QFLOAT8     # fp8 (requires opset 19+)\n",
    "```\n",
    "\n",
    "## Per-Channel vs Per-Tensor\n",
    "\n",
    "**Per-tensor**: single scale/zero-point per layer (faster)\n",
    "**Per-channel**: separate scale/zero-point per channel (more accurate)\n",
    "\n",
    "Use per-channel for weights in conv layers for better accuracy.\n",
    "\n",
    "## Model Requirements\n",
    "\n",
    "- **Opset 10+** (recommend 13+)\n",
    "- INT4 needs opset 21+\n",
    "- FP8 needs opset 19+\n",
    "- Models > 2GB need `use_external_data_format=True`\n",
    "\n",
    "## Benchmarking\n",
    "\n",
    "```python\n",
    "import time, onnxruntime as ort, numpy as np\n",
    "\n",
    "def benchmark(model_path, input_shape, num_runs=100):\n",
    "    sess = ort.InferenceSession(model_path)\n",
    "    x = np.random.rand(*input_shape).astype(np.float32)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    \n",
    "    sess.run(None, {input_name: x})  # warm-up\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        sess.run(None, {input_name: x})\n",
    "    \n",
    "    return (time.time() - start) / num_runs\n",
    "\n",
    "orig_time = benchmark('model.onnx', (1, 3, 416, 416))\n",
    "quant_time = benchmark('model_quant.onnx', (1, 3, 416, 416))\n",
    "\n",
    "print(f\"Speedup: {orig_time/quant_time:.2f}x\")\n",
    "```\n",
    "\n",
    "## Hardware-Specific\n",
    "\n",
    "**CPU (AVX2/AVX-512):**\n",
    "- Use U8U8 or U8S8 format\n",
    "- activation_type=QuantType.QUInt8\n",
    "\n",
    "**GPU (TensorRT):**\n",
    "- Need Tensor Core support (T4, A100)\n",
    "- TensorRT handles quantization logic\n",
    "- Pass full precision model + calibration results\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```python\n",
    "# 1. Prune\n",
    "model = onnx.load('model.onnx')\n",
    "pruned = prune_model(model, 0.01)\n",
    "onnx.save(pruned, 'model_pruned.onnx')\n",
    "\n",
    "# 2. Optimize\n",
    "quant_pre_process('model_pruned.onnx', 'model_opt.onnx')\n",
    "\n",
    "# 3. Calibrate & Quantize\n",
    "calib_reader = MyCalibrationDataReader('calib_data/')\n",
    "quantize_static(\n",
    "    'model_opt.onnx',\n",
    "    'model_quant.onnx',\n",
    "    calib_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "\n",
    "# 4. Benchmark\n",
    "# (see above)\n",
    "```\n",
    "\n",
    "## vs Dynamic Quantization\n",
    "\n",
    "| | Static | Dynamic |\n",
    "|---|---|---|\n",
    "| Calibration | Required | No |\n",
    "| Setup | Complex | Simple |\n",
    "| Speed | Faster | Good |\n",
    "| Accuracy | Better | Good |\n",
    "| Hardware | GPU/TPU/CPU | CPU only |\n",
    "| Production | Yes | Prototype |\n",
    "\n",
    "## When to Use\n",
    "\n",
    "✅ Use static when:\n",
    "- Deploying to production\n",
    "- Need max speed\n",
    "- Have calibration data\n",
    "- Targeting GPU/TPU\n",
    "- Working with CNNs\n",
    "\n",
    "❌ Don't use when:\n",
    "- No calibration data\n",
    "- Quick prototyping\n",
    "- Input distributions vary wildly\n",
    "\n",
    "## Considerations\n",
    "\n",
    "- Calibration data MUST be representative of real data\n",
    "- Poor calibration = bad accuracy\n",
    "- Models > 2GB need external data format\n",
    "- Per-channel quant helps accuracy but uses more memory\n",
    "- Always validate accuracy after quantizing\n",
    "\n",
    "## Debugging\n",
    "\n",
    "If accuracy drops:\n",
    "- Increase calibration dataset size\n",
    "- Try per-channel quantization\n",
    "- Check if certain layers causing issues\n",
    "- Consider QAT (quantization-aware training) instead\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```bash\n",
    "pip install onnxruntime onnxoptimizer\n",
    "```\n",
    "\n",
    "```python\n",
    "from onnxruntime.quantization import quantize_static, QuantType\n",
    "\n",
    "quantize_static(\n",
    "    'model.onnx',\n",
    "    'model_quant.onnx',\n",
    "    calibration_data_reader=my_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QUInt8,\n",
    "    per_channel=False\n",
    ")\n",
    "```\n",
    "\n",
    "Static = better performance, more work. Worth it for production deployment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
