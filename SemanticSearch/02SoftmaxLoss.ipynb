{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classification loss \n",
    "- Not used in practice\n",
    "\n",
    "- Models are trained on either siamese network or a triplet network. \n",
    "\n",
    "### Siamese netowrk\n",
    "- two copies of the same BERT\n",
    "- Sentence A and B are fed into BERT model to produce token embedding \n",
    "- Pooling operation is used to create sentence embedding (mean average pooling)\n",
    "- Goal is to optimize to get sentence embedds as close as possible for similar sentences \n",
    "- Sentence vectors are concatinated to create vector (u, v, |u-v|) with dimensionality of 768 * 3\n",
    "- The concatinated vector is fed into a FFNN\n",
    "- FFNN output is 3 output activations \n",
    "    - Within the NLI training data there are entailment, nuetral, and contridiction \n",
    "- Output is the \"true\" label \n",
    "- Optomized using cross enthropy loss which contains a softmax loss function within  \n",
    "\n",
    "### Multiple connectors ranking loss (MNR)\n",
    "- Used in practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 550152\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets \n",
    "# SNLI: Stanford Natural Language Inference dataset \n",
    "snli = datasets.load_dataset('snli', split = 'train')\n",
    "snli\n",
    "#Sentence A is premise, B is the hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label'],\n",
       "    num_rows: 942069\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNLI comes from the glue dataset\n",
    "mnli = datasets.load_dataset('glue', 'mnli', split='train')\n",
    "mnli\n",
    "#mnli has an extra column for index\n",
    "#In order to merge the datasets we need to reformat the mnli dataset \n",
    "mnli = mnli.remove_columns(['idx'])\n",
    "\n",
    "snli = snli.cast(mnli.features)             #change schema using cast so we can combine dataset \n",
    "\n",
    "dataset = datasets.concatenate_datasets([snli,mnli])\n",
    "dataset                     #contains 942854 rows including error rows\n",
    "#rows with -1 are errors, therfore want to remove. \n",
    "\n",
    "dataset = dataset.filter(\n",
    "    lambda x: False if x['label'] == -1 else True           #function selects rows where the label value is not -1, keeps others \n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample #Input example is just a data format used by the library \n",
    "from tqdm.auto import tqdm                     #for progress bar \n",
    "\n",
    "train_samples = []\n",
    "for row in tqdm(dataset):\n",
    "    train_samples.append(InputExample(\n",
    "        texts=[row['premise'], row['hypothesis']],  #the input processed into the model \n",
    "        label = row['label']                        #these are feature names from our dataset  \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Require a dataloader (sometimes you can use dataloaders from the sentence tranformer library)\n",
    "#Using pytorch as dataloader \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "batch_size = 16 \n",
    "loader = DataLoader(train_samples, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "#Initialize the model using sentence transformers \n",
    "# ST uses modules to settle the model. will have a trasformer module and pooling module for mean pooling layer \n",
    " \n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "\n",
    "bert = models.Transformer('bert-base-uncased') #using the hugging face model \n",
    "pooler = models.Pooling(bert.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "\n",
    "model = SentenceTransformer(modules=[bert, pooler]) #initialize, put sentence transformer name in the ()\n",
    "model\n",
    "#outputs sentence tranformer \"structure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize loss function \n",
    "from sentence_transformers import losses \n",
    "\n",
    "loss = losses.SoftmaxLoss(\n",
    "    model = model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels =3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear CUDA memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory usage limits\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)  # Use 80% of GPU memory\n",
    "\n",
    "# Move model to appropriate device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "print(f'moved to {device}')\n",
    "\n",
    "# Print available GPU memory for debugging\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print(f'GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB')\n",
    "    print(f'GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def get_device_with_memory_check(model, required_memory_gb=4):\n",
    "    if torch.cuda.is_available():\n",
    "        # Check available GPU memory\n",
    "        gpu = torch.cuda.current_device()\n",
    "        gpu_properties = torch.cuda.get_device_properties(gpu)\n",
    "        total_memory = gpu_properties.total_memory / 1e9  # Convert to GB\n",
    "        allocated_memory = torch.cuda.memory_allocated(gpu) / 1e9\n",
    "        free_memory = total_memory - allocated_memory\n",
    "        \n",
    "        if free_memory > required_memory_gb:\n",
    "            print(f'Using GPU with {free_memory:.2f}GB free memory')\n",
    "            return torch.device('cuda')\n",
    "        else:\n",
    "            print(f'GPU memory low ({free_memory:.2f}GB free), falling back to CPU')\n",
    "            return torch.device('cpu')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "# Use it before training\n",
    "device = get_device_with_memory_check(model)\n",
    "model.to(device)\n",
    "\n",
    "# Then train\n",
    "epochs = 1\n",
    "warmup_steps = int(len(loader)*epochs*0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=epochs, \n",
    "    warmup_steps=warmup_steps, \n",
    "    output_path = '/sbert_test_b',\n",
    "    show_progress_bar = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean pooling function using pytorch \n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    pool = torch.sum(token_embeds*in_mask, 1) /torch.clamp(         #pooling method takes token embeddings and compresses into a single 768 dimentional vector (from 512)\n",
    "        in_mask.sum(1), min=1e-9)\n",
    "    return pool\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
